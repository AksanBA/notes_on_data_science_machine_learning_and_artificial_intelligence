<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Chris Albon</title><link>http://chrisalbon.com/</link><description></description><lastBuildDate>Tue, 06 Sep 2016 12:00:00 -0700</lastBuildDate><item><title>Construct A Dictionary From Multiple Lists</title><link>http://chrisalbon.com/python/construct_a_dictionary_from_multiple_lists_python.html</link><description>&lt;h2&gt;Create Two Lists&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create a list of theofficer&amp;#39;s name&lt;/span&gt;
&lt;span class="n"&gt;officer_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Sodoni Dogla&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Chris Jefferson&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Jessica Billars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Michael Mulligan&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Steven Johnson&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Create a list of the officer&amp;#39;s army&lt;/span&gt;
&lt;span class="n"&gt;officer_armies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Purple Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Orange Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Green Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Red Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blue Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Construct A Dictionary From The Two Lists&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create a dictionary that is the zip of the two lists&lt;/span&gt;
&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;officer_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;officer_armies&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Tue, 06 Sep 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-09-06:python/construct_a_dictionary_from_multiple_lists_python.html</guid><category>Basics</category></item><item><title>Exiting A Loop</title><link>http://chrisalbon.com/python/exiting_a_loop_python.html</link><description>&lt;h2&gt;Create A List&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create a list:&lt;/span&gt;
&lt;span class="n"&gt;armies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Red Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blue Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Green Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Breaking Out Of A For Loop&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;army&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;armies&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;army&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;army&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blue Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Blue Army Found! Stopping.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Red Army
Blue Army
Blue Army Found! Stopping.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that the loop stopped after the conditional if statement was satisfied.&lt;/p&gt;
&lt;h2&gt;Exiting If Loop Completed&lt;/h2&gt;
&lt;p&gt;A loop will exit when completed, but using an &lt;code&gt;else&lt;/code&gt; statement we can add an action at the conclusion of the loop if it hasn't been exited earlier.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;army&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;armies&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;army&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;army&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Orange Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Looped Through The Whole List, No Orange Army Found&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Red Army
Blue Army
Green Army
Looped Through The Whole List, No Orange Army Found
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Tue, 06 Sep 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-09-06:python/exiting_a_loop_python.html</guid><category>Basics</category></item><item><title>Iterating Over Dictionary Keys</title><link>http://chrisalbon.com/python/iterating_over_dictionary_keys_python.html</link><description>&lt;h2&gt;Create A Dictionary&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Officers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Michael Mulligan&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Red Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;Steven Johnson&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blue Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;Jessica Billars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Green Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;Sodoni Dogla&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Purple Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;Chris Jefferson&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Orange Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Officers&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;#39;Chris Jefferson&amp;#39;: &amp;#39;Orange Army&amp;#39;,
 &amp;#39;Jessica Billars&amp;#39;: &amp;#39;Green Army&amp;#39;,
 &amp;#39;Michael Mulligan&amp;#39;: &amp;#39;Red Army&amp;#39;,
 &amp;#39;Sodoni Dogla&amp;#39;: &amp;#39;Purple Army&amp;#39;,
 &amp;#39;Steven Johnson&amp;#39;: &amp;#39;Blue Army&amp;#39;}
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Use Dictionary Comprehension&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Display all dictionary entries where the key doesn&amp;#39;t start with &amp;#39;Chris&amp;#39;&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Officers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;keys&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;Officers&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;startswith&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Chris&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;#39;Jessica Billars&amp;#39;: &amp;#39;Green Army&amp;#39;,
 &amp;#39;Michael Mulligan&amp;#39;: &amp;#39;Red Army&amp;#39;,
 &amp;#39;Sodoni Dogla&amp;#39;: &amp;#39;Purple Army&amp;#39;,
 &amp;#39;Steven Johnson&amp;#39;: &amp;#39;Blue Army&amp;#39;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that the entry for 'Chris Jefferson' is not returned.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Tue, 06 Sep 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-09-06:python/iterating_over_dictionary_keys_python.html</guid><category>Basics</category></item><item><title>Looping Over Two Lists</title><link>http://chrisalbon.com/python/looping_over_two_lists_using_Python.html</link><description>&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create a list of length 3:&lt;/span&gt;
&lt;span class="n"&gt;armies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Red Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blue Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Green Army&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Create a list of length 4:&lt;/span&gt;
&lt;span class="n"&gt;units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Red Infantry&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blue Armor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Green Artillery&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Orange Aircraft&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# For each element in the first list,&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;army&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unit&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;armies&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Display the corresponding index element of the second list:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;army&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;has the following options:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Red Army has the following options: Red Infantry
Blue Army has the following options: Blue Armor
Green Army has the following options: Green Artillery
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that the fourth item of the second list, &lt;code&gt;orange aircraft&lt;/code&gt;, did not display.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Tue, 06 Sep 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-09-06:python/looping_over_two_lists_using_Python.html</guid><category>Basics</category></item><item><title>K-Nearest Neighbors Classification</title><link>http://chrisalbon.com/machine-learning/k-nearest_neighbors_using_scikit_pandas.html</link><description>&lt;p&gt;K-nearest neighbors classifier (KNN) is a simple and powerful classification learner.&lt;/p&gt;
&lt;p&gt;KNN has three basic parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;: The class of an observation (what we are trying to predict in the test data).&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(X_i\)&lt;/span&gt;: The predictors/IVs/attributes of an observation.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(K\)&lt;/span&gt;: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its "neighborhood". For example, K=2 means that each observation's has a neighorhood comprising of the two other observations closest to it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Imagine we have an observation where we know its independent variables &lt;span class="math"&gt;\(x_{test}\)&lt;/span&gt; but do not know its class &lt;span class="math"&gt;\(y_{test}\)&lt;/span&gt;. The KNN learner finds the K other observations that are closest to &lt;span class="math"&gt;\(x_{test}\)&lt;/span&gt; and uses their known classes to assign a classes to &lt;span class="math"&gt;\(x_{test}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;neighbors&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create Dataset&lt;/h2&gt;
&lt;p&gt;Here we create three variables, &lt;code&gt;test_1&lt;/code&gt; and &lt;code&gt;test_2&lt;/code&gt; are our independent variables, 'outcome' is our dependent variable. We will use this data to train our learner.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.3051&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.4949&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.6974&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.3769&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.2231&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.341&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.4436&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5897&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.6308&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.5846&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.2654&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.2615&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.4538&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.4615&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.8308&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.4962&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.3269&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.5346&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.6731&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;outcome&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;win&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;win&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;win&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;win&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;win&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;test_1&lt;/th&gt;
      &lt;th&gt;test_2&lt;/th&gt;
      &lt;th&gt;outcome&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.3051&lt;/td&gt;
      &lt;td&gt;0.5846&lt;/td&gt;
      &lt;td&gt;win&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.4949&lt;/td&gt;
      &lt;td&gt;0.2654&lt;/td&gt;
      &lt;td&gt;win&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.6974&lt;/td&gt;
      &lt;td&gt;0.2615&lt;/td&gt;
      &lt;td&gt;win&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.3769&lt;/td&gt;
      &lt;td&gt;0.4538&lt;/td&gt;
      &lt;td&gt;win&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.2231&lt;/td&gt;
      &lt;td&gt;0.4615&lt;/td&gt;
      &lt;td&gt;win&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2&gt;Plot the data&lt;/h2&gt;
&lt;p&gt;This is not necessary, but because we only have three variables, we can plot the training dataset. The X and Y axes are the independent variables, while the colors of the points are their classes.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lmplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;test_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fit_reg&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;hue&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;outcome&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scatter_kws&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;marker&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;D&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;seaborn.axisgrid.FacetGrid at 0x11553bf98&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://chrisalbon.com/images/k-nearest_neighbors_classifer/output_9_1.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Convert Data Into np.arrays&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;scikit-learn&lt;/code&gt; library requires the data be formatted as a &lt;code&gt;numpy&lt;/code&gt; array. Here are doing that reformatting.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;test_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;outcome&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Train The Learner&lt;/h2&gt;
&lt;p&gt;This is our big moment. We train a KNN learner using the parameters that an observation's neighborhood is its three closest neighors. &lt;code&gt;weights = 'uniform'&lt;/code&gt; can be thought of as the voting system used. For example, &lt;code&gt;uniform&lt;/code&gt; means that all neighbors get an equally weighted "vote" about an observation's class while &lt;code&gt;weights = 'distance'&lt;/code&gt; would tell the learner to weigh each observation's "vote" by its distance from the observation we are classifying.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;uniform&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;trained_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;View The Model's Score&lt;/h2&gt;
&lt;p&gt;How good is our trained model compared to our training data?&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;trained_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0.80000000000000004
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our model is 80% accurate!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: that in any real world example we'd want to compare the trained model to some holdout test data. But since this is a toy example I used the training data&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Apply The Learner To A New Data Point&lt;/h2&gt;
&lt;p&gt;Now that we have trained our model, we can predict the class any new observation, &lt;span class="math"&gt;\(y_{test}\)&lt;/span&gt;. Let us do that now!&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create a new observation with the value of the first independent variable, &amp;#39;test_1&amp;#39;, as .4&lt;/span&gt;
&lt;span class="c1"&gt;# and the second independent variable, test_1&amp;#39;, as .6&lt;/span&gt;
&lt;span class="n"&gt;x_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Apply the learner to the new, unclassified observation.&lt;/span&gt;
&lt;span class="n"&gt;trained_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;array([&amp;#39;loss&amp;#39;], dtype=object)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Huzzah! We can see that the learner has predicted that the new observation's class is &lt;code&gt;loss&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can even look at the probabilities the learner assigned to each class:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;trained_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;array([[ 0.66666667,  0.33333333]])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;According to this result, the model predicted that the observation was &lt;code&gt;loss&lt;/code&gt; with a ~67% probability and &lt;code&gt;win&lt;/code&gt; with a ~33% probability. Because the observation had a greater probability of being &lt;code&gt;loss&lt;/code&gt;, it predicted that class for the observation.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The choice of K has major affects on the classifer created.&lt;/li&gt;
&lt;li&gt;The greater the K, more linear (high bias and low variance) the decision boundary.&lt;/li&gt;
&lt;li&gt;There are a variety of ways to measure distance, two popular being simple euclidean distance and cosine similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Wed, 31 Aug 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-08-31:machine-learning/k-nearest_neighbors_using_scikit_pandas.html</guid><category>Basics</category></item><item><title>Bessel's Correction</title><link>http://chrisalbon.com/frequentist-statistics/bessels_correction.html</link><description>&lt;p&gt;Bessel's correction is the reason we use &lt;span class="math"&gt;\(n-1\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(n\)&lt;/span&gt; in the calculations of sample variance and sample standard deviation.&lt;/p&gt;
&lt;p&gt;Sample variance:&lt;/p&gt;
&lt;div class="math"&gt;$$ s^2 = \frac {1}{n-1} \sum_{i=1}^n  \left(x_i - \overline{x} \right)^ 2 $$&lt;/div&gt;
&lt;p&gt;When we calculate sample variance, we are attempting to estimate the population variance, an unknown value. To make this estimate, we estimate this unknown population variance from the mean of the squared deviations of samples from the overall sample mean. A negative sideffect of this estimation technique is that, because we are taking a sample, we are a more likely to observe observations with a smaller deviation because they are more common (e.g. they are the center of the distribution). The means that by definiton we will underestimate the population variance.&lt;/p&gt;
&lt;p&gt;Friedrich Bessel figured out that by multiplying a biased (uncorrected) sample variance &lt;span class="math"&gt;\(s_n^2 = \frac {1}{n} \sum_{i=1}^n  \left(x_i - \overline{x} \right)^ 2\)&lt;/span&gt; by &lt;span class="math"&gt;\(\frac{n}{n-1}\)&lt;/span&gt; we will be able to reduce that bias and thus be able to make an accurate estimate of the population variance and standard deviation. The end result of that multiplication is the unbiased sample variance.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Fri, 19 Aug 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-08-19:frequentist-statistics/bessels_correction.html</guid><category>Basics</category></item><item><title>Linear Regression In scikit-learn</title><link>http://chrisalbon.com/machine-learning/linear_regression_scikit-learn.html</link><description>&lt;p&gt;Sources: &lt;a href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#example-linear-model-plot-ols-py"&gt;scikit-learn&lt;/a&gt;, &lt;a href="http://robertgrantstats.co.uk/drawmydata.html"&gt;DrawMyData&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of this tutorial is to give a brief introduction into the logic of statistical model building used in machine learning. If you want to read more about the theory behind this tutorial, check out &lt;a href="https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370"&gt;An Introduction To Statistical Learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let us get started.&lt;/p&gt;
&lt;h2&gt;Preliminary&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Load Data&lt;/h2&gt;
&lt;p&gt;With those libraries added, let us load the dataset (the dataset is avaliable in his site's GitHub repo).&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Load the data&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;../data/simulated_data/battledeaths_n300_cor99.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Shuffle the data&amp;#39;s rows (This is only necessary because of the way I created&lt;/span&gt;
&lt;span class="c1"&gt;# the data using DrawMyData. This would not normally be necessary with a real analysis).&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frac&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;hi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Explore Data&lt;/h2&gt;
&lt;p&gt;Let us take a look at the first few rows of the data just to get an idea about it.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# View the first few rows&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;friendly_battledeaths&lt;/th&gt;
      &lt;th&gt;enemy_battledeaths&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;173&lt;/th&gt;
      &lt;td&gt;28.4615&lt;/td&gt;
      &lt;td&gt;26.5385&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;257&lt;/th&gt;
      &lt;td&gt;9.7436&lt;/td&gt;
      &lt;td&gt;7.6923&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;162&lt;/th&gt;
      &lt;td&gt;20.0000&lt;/td&gt;
      &lt;td&gt;10.3846&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;12.8205&lt;/td&gt;
      &lt;td&gt;10.3846&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;111&lt;/th&gt;
      &lt;td&gt;63.8462&lt;/td&gt;
      &lt;td&gt;61.9231&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now let us plot the data so we can see it's structure.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Plot the two variables against eachother&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;friendly_battledeaths&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;enemy_battledeaths&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;scatter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x11a9fc710&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="http://chrisalbon.com/images/linear_regression_scikitlearn/output_12_1.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Break Data Up Into Training And Test Datasets&lt;/h2&gt;
&lt;p&gt;Now for the real work. To judge how how good our model is, we need something to test it against. We can accomplish this using a technique called cross-validation. Cross-validation can get much more complicated and powerful, but in this example we are going do the most simple version of this technique.&lt;/p&gt;
&lt;h3&gt;Steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Divide the dataset into two datasets: A 'training' dataset that we will use to train our model and a 'test' dataset that we will use to judge the accuracy of that model.&lt;/li&gt;
&lt;li&gt;Train the model on the 'training' data.&lt;/li&gt;
&lt;li&gt;Apply that model to the test data's X variable, creating the model's guesses for the test data's Ys.&lt;/li&gt;
&lt;li&gt;Compare how close the model's predictions for the test data's Ys were to the actual test data Ys.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create our predictor/independent variable&lt;/span&gt;
&lt;span class="c1"&gt;# and our response/dependent variable&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;friendly_battledeaths&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;enemy_battledeaths&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Create our test data from the first 30 observations&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Create our training data from the remaining observations&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Train The Linear Model&lt;/h2&gt;
&lt;p&gt;Let us train the model using our training data.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create an object that is an ols regression&lt;/span&gt;
&lt;span class="n"&gt;ols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linear_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Train the model using our training data&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ols&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;View The Results&lt;/h2&gt;
&lt;p&gt;Here are some basic outputs of the model, notably the coefficient and the R-squared score.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# View the training model&amp;#39;s coefficient&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;array([ 0.9770556])
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# View the R-Squared score&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0.98719951914847326
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have used the training data to train a model, called &lt;code&gt;model&lt;/code&gt;, we can apply it to the test data's Xs to make predictions of the test data's Ys.&lt;/p&gt;
&lt;p&gt;Previously we used &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt; to train a linear regression model, which we stored as a variable called &lt;code&gt;model&lt;/code&gt;. The code &lt;code&gt;model.predict(X_test)&lt;/code&gt; applies the trained model to the &lt;code&gt;X_test&lt;/code&gt; data, data the model has never seen before to make predicted values of Y.&lt;/p&gt;
&lt;p&gt;This can easily be seen by simply running the code:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Run the model on X_test and show the first five results&lt;/span&gt;
&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[27.238901424783169,
 8.9504723858776192,
 18.971545454685064,
 11.956774765407825,
 61.811720758841012]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This array of values is the model's best guesses for the values of the test data's Ys. Compare them to the actual test data Y values:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# View the first five test Y values&lt;/span&gt;
&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[26.538499999999999,
 7.6923000000000004,
 10.384600000000001,
 10.384600000000001,
 61.923099999999998]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our model predicts that the first observation in the test data will have a Y value of 27.24. Actual Y value for that observation was 26.54. That means the model's prediction was &lt;span class="math"&gt;\(27.24 - 26.54 = 0.8\)&lt;/span&gt; off, this is called the residual of that observation.&lt;/p&gt;
&lt;p&gt;The difference between the model's predicted values and the actual values is how is we judge as model's accuracy, because a perfectly accurate model would have residuals of zero.&lt;/p&gt;
&lt;p&gt;However, to judge a model, we want a single statistic (number) that we can use as a measure. We want this measure to capture the difference between the predicted values and the actual values across all observations in the data.&lt;/p&gt;
&lt;p&gt;The most common statistic used for quantitative Ys is the &lt;strong&gt;residual sum of squares&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$ RSS = \sum_{i=1}^{n}(y_{i}-f(x_{i}))^{2} $$&lt;/div&gt;
&lt;p&gt;Don't let the mathematical notation throw you off:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(f(x_{i})\)&lt;/span&gt; is the model we trained: &lt;code&gt;model.predict(X_test)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(y_{i}\)&lt;/span&gt; is the test data's y: &lt;code&gt;y_test&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(^{2}\)&lt;/span&gt; is the exponent: &lt;code&gt;**2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\sum_{i=1}^{n}\)&lt;/span&gt; is the summation: &lt;code&gt;.sum()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the residual sum of squares, for each observation we find the difference between the model's predicted Y and the actual Y, then square that difference to make all the values positive. Then we add all those squared differences together to get a single number. The final result is a statistic representing how far the model's predictions were from the real values.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Apply the model we created using the training data&lt;/span&gt;
&lt;span class="c1"&gt;# to the test data, and calculate the RSS.&lt;/span&gt;
&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;352.1634102396179
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: You can also use Mean Squared Error, which is RSS divided by the degrees of freedom. But I find it helpful to think in terms of RSS.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Calculate the MSE&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;11.738780341320597
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What does the model's RSS of 352.16 mean? Mathematically, it is the sum of the squared errors (obviously). But substantly 352.16 has little real meaning. Then why is RSS so fundamental to everything we do? &lt;strong&gt;Because it lets us compare models.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Does 352.16 mean our model is good? On it's own we don't realy have a good answer. But what if we trained a second model -- with different independent variables -- and applied that model to the same test data and got an RSS of 200? Then we would know that the second model is better! And that hunt for the best model is very often our goal.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Fri, 19 Aug 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-08-19:machine-learning/linear_regression_scikit-learn.html</guid><category>Basics</category></item><item><title>Basic Mathematical Terms In Python</title><link>http://chrisalbon.com/mathematics/basic_mathematical_terms_in_python.html</link><description>&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Ratio&lt;/h3&gt;
&lt;p&gt;Two quantities divided by eachother, &lt;span class="math"&gt;\(\frac{x}{y}\)&lt;/span&gt;. Often written as &lt;span class="math"&gt;\(x:y\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0.6666666666666666
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Proportion&lt;/h3&gt;
&lt;p&gt;One variable divided by the sum of itself and another variable: &lt;span class="math"&gt;\(\left | \frac{x}{x+y} \right |\)&lt;/span&gt;. Proportions range from 0 to 1.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0.4
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Percentage&lt;/h3&gt;
&lt;p&gt;Proportions multiplied by 100: $\left | \frac{x}{x+y} \right | \times 100 $&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;40.0
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Functions&lt;/h3&gt;
&lt;p&gt;Functions assigns something to each element of a domain. &lt;span class="math"&gt;\(f\left ( x \right ): A \rightarrow B\)&lt;/span&gt; means "Function x maps A to B"&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="c1"&gt;# Create a function called x&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_range&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;_range&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;9
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Exponential Function&lt;/h3&gt;
&lt;p&gt;Multiplication of a number by itself. Example: &lt;span class="math"&gt;\(y^{3} = y \times y \times y\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="c1"&gt;# y*y*y&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;27
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Logarithmic Function&lt;/h3&gt;
&lt;p&gt;Inverse of exponential function, transforming exponential functions to linear functions. Logarithms tell you how many times to multiple a base to get a value. Example: &lt;span class="math"&gt;\(log_{a}a^{x} = x\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;

&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;4.0
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Natural Log&lt;/h3&gt;
&lt;p&gt;Logarithmic Function with a base of &lt;span class="math"&gt;\(e\)&lt;/span&gt;, where &lt;span class="math"&gt;\(e\approx 2.7183\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1.3862943611198906
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Sequence&lt;/h3&gt;
&lt;p&gt;An ordered list of numbers. Example: &lt;span class="math"&gt;\(x = \left \{ 1,5,100,2,5 \right \}\)&lt;/span&gt;. The &lt;span class="math"&gt;\(ith\)&lt;/span&gt; individual member of a sequence, &lt;span class="math"&gt;\(x\)&lt;/span&gt; is denoted &lt;span class="math"&gt;\(x_{i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[1, 5, 100, 2, 5]
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Series&lt;/h3&gt;
&lt;p&gt;The sum of a sequence, or put another way, a sequence with plus signs between elements. Example: &lt;span class="math"&gt;\(\sum_{i=1}^{n}x_{i}\)&lt;/span&gt; is the sum of the first to the &lt;span class="math"&gt;\(nth\)&lt;/span&gt; member of a sequence, &lt;span class="math"&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;113
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Derivative&lt;/h3&gt;
&lt;p&gt;The instanteous rate of change of a function. Can be thought of as the slope between two points, &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; as the distance between them approaches being infinitely small.&lt;/p&gt;
&lt;h3&gt;Definite integral&lt;/h3&gt;
&lt;p&gt;The area under a curve.&lt;/p&gt;
&lt;h3&gt;Local extrema, local maxima, local minima&lt;/h3&gt;
&lt;p&gt;A high or low point of a function.&lt;/p&gt;
&lt;h3&gt;Global extrema, global maxima, global minima&lt;/h3&gt;
&lt;p&gt;The highest or lowest point of a function.&lt;/p&gt;
&lt;h3&gt;Taylor Series&lt;/h3&gt;
&lt;p&gt;Describing a function by using the information contained in the deriviatives of the function.&lt;/p&gt;
&lt;h3&gt;Critical Point&lt;/h3&gt;
&lt;p&gt;Any point in a function where the first derivative is 0 or doesn't exist. Basically any point where something interesting might happen.&lt;/p&gt;
&lt;h3&gt;Inflection Point&lt;/h3&gt;
&lt;p&gt;A point of a function where the curve switches from concave to convex or vice versa.&lt;/p&gt;
&lt;h3&gt;Stationary Point&lt;/h3&gt;
&lt;p&gt;A point of a function where the first derivative is 0. Can be thought of as either a peak or a valley floor.&lt;/p&gt;
&lt;h3&gt;Saddle Point&lt;/h3&gt;
&lt;p&gt;A point of a functon where both the first and second derivative is 0.&lt;/p&gt;
&lt;h3&gt;Concave&lt;/h3&gt;
&lt;p&gt;A function where the rate of increase slows as the value of the function gets bigger. Another way to think about it: if plotted in 2d, a convex function curves towards the ground.&lt;/p&gt;
&lt;h3&gt;Convex&lt;/h3&gt;
&lt;p&gt;A function where the rate of increase speeds up as the value of the function gets bigger. Another way to think about it: if plotted in 2d, a convex function curves towards the sky.&lt;/p&gt;
&lt;h3&gt;Conditional Probability&lt;/h3&gt;
&lt;p&gt;The probability an event occurs given whether or not another event occurs.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Pr\left ( y\mid x = 4 \right )\)&lt;/span&gt; is the probability y occurs given that the event x=4 occurs.&lt;/p&gt;
&lt;h3&gt;Combination&lt;/h3&gt;
&lt;p&gt;From n items, choose k items, ignoring the order in which you choose them: &lt;span class="math"&gt;\(\binom{n}{k}\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Odds and Odds Ratios&lt;/h3&gt;
&lt;p&gt;The odds of an event are the probability of an event occurs over the probability of an event not occuring: &lt;span class="math"&gt;\(\frac{Pr(x)}{Pr(~x)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The odds ratio of two events, &lt;span class="math"&gt;\(x\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; are the odds of the two events:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\frac{Pr(x)/Pr(~x)}{Pr(y)/Pr(~y)}\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Random Variable&lt;/h3&gt;
&lt;p&gt;A random variable is a variable that can take an array of values, with the probability it takes a particular value determined by some random process.&lt;/p&gt;
&lt;p&gt;A distribution of a random variable is the probability of each possible value being realized.&lt;/p&gt;
&lt;h3&gt;Probability Mass Function &amp;amp; Probability Distribution Function&lt;/h3&gt;
&lt;p&gt;A function that defines the probability of each possible value occuring in a discrete (PMF) or continous (PDF) function.&lt;/p&gt;
&lt;h3&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;p&gt;Applies only to binary random variables only (e.g. coin flips). It's PMF is written as:&lt;/p&gt;
&lt;div class="math"&gt;$$ Pr(Y = y\mid p)=\begin{cases}
1-p &amp;amp; \text{ for } y = 0\\
p &amp;amp; \text{ for } y= 1
\end{cases} $$&lt;/div&gt;
&lt;p&gt;The Bernoulli distribution describes the ferquencey of two outcomes over repeated observations. It is built upon the assumption that the events of independent of eachother, meaning the outcome of one coin flip doesn't not affect the outcome of the second coin flip (from Moore and Siegel).&lt;/p&gt;
&lt;h3&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The PMF of the binomial distribution is:&lt;/p&gt;
&lt;div class="math"&gt;$$ Pr(Y = y\mid n,p) = \binom{n}{y}p^{y}(1-p)^{n-y}$$&lt;/div&gt;
&lt;p&gt;The binomial distribution describes any discrete distribution with three or more observations where (1) each observation is composed of a binary outcome, (2) the observations are independent, and (3) we have a record of the number of times one value was obtained (from Moore and Siegel).&lt;/p&gt;
&lt;h3&gt;Poisson Distribution&lt;/h3&gt;
&lt;p&gt;The Poisson distribution the number of times you observe one event, two events, three events, etc over a fixed period of time. Put another way, it describes the distribution of event counts for rare, random events (from Moore and Siegel).&lt;/p&gt;
&lt;div class="math"&gt;$$Pr(Y=y|u)=\frac{u^y}{y!\times e^u}$$&lt;/div&gt;
&lt;p&gt;For example, the Poisson distribution would describe the probability of observing 10 wars in a century.&lt;/p&gt;
&lt;h3&gt;Negative Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The NB distribution describes the number of events occuring prior to observing the kth non-event. For example, the number of successful surgeries performed before the 6th failed surgery.&lt;/p&gt;
&lt;h3&gt;Expectation Of Random Variables&lt;/h3&gt;
&lt;p&gt;The most likely value a random variable takes. Written &lt;span class="math"&gt;\(E[X]\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Moments Of A Distribution&lt;/h3&gt;
&lt;p&gt;Moments of a distribution are the parameters used to describe a distribution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First moment: Mean&lt;/li&gt;
&lt;li&gt;Second moment: Variance&lt;/li&gt;
&lt;li&gt;Third moment: Skewness&lt;/li&gt;
&lt;li&gt;Fourth moment: Kurtosis (flatness or peakedness)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Uniform Distribution&lt;/h3&gt;
&lt;p&gt;Assigns an equal probability to all possible events.&lt;/p&gt;
&lt;h3&gt;Gaussian-Normal Distribution&lt;/h3&gt;
&lt;p&gt;The bell curve. Defined by two moments: mean and variance.&lt;/p&gt;
&lt;h3&gt;Logistic Distribution&lt;/h3&gt;
&lt;p&gt;Often used to model binary outcomes. Defined by two moments: mean and variance.&lt;/p&gt;
&lt;h3&gt;Exponential Distribution&lt;/h3&gt;
&lt;p&gt;The exponential distribution describes events produced by a process with a constant risk of failure (from Moore and Siegel).&lt;/p&gt;
&lt;h3&gt;Pareto Distribution&lt;/h3&gt;
&lt;p&gt;A distribution in which prior values affect later values.&lt;/p&gt;
&lt;h3&gt;Gamma Distribution&lt;/h3&gt;
&lt;p&gt;While the exponential distribution assumes contant risk, the Gamma distribution allows risk to vary over a number of periods.&lt;/p&gt;
&lt;h3&gt;Weibull Distribution&lt;/h3&gt;
&lt;p&gt;"[Weibull distribution] can be interpreted as if several processes are running in parallel, with the first to stop ending the duration. This is the weakest link mechanism, as when the failure of some part causes a machine to break down and the total operating time of the machine is the duration." (Lindsey 1995, p. 133)&lt;/p&gt;
&lt;h3&gt;Chi-squared Distribution&lt;/h3&gt;
&lt;p&gt;"The sum of squares of n independent variables each distributed according to a standard normal distribution is distributed according to a chi-squared distribution." (Moore and Siegel)&lt;/p&gt;
&lt;h3&gt;Student's t Distribution&lt;/h3&gt;
&lt;p&gt;This distribution looks like a normal distribution when N is big enough, but with thicker tails when N is small.&lt;/p&gt;
&lt;h3&gt;Vector&lt;/h3&gt;
&lt;p&gt;An 'arrow' in n-dimensional space. For example, in a 2d coordinate plane, the vector (2,3) is x=2 and y=3.&lt;/p&gt;
&lt;h3&gt;Length Of A Vector&lt;/h3&gt;
&lt;p&gt;If you think of a 2d coordinate plane, and have a vector (x,y), then you have a point at x,y. The distance from 0 (the origin) to x,y is the vector's length.&lt;/p&gt;
&lt;p&gt;The length of vector &lt;span class="math"&gt;\(a\)&lt;/span&gt; is denoted &lt;span class="math"&gt;\(\left \| a \right \|\)&lt;/span&gt; and is calculated &lt;span class="math"&gt;\(\sqrt{a_{1}^{2}+a_{2}^{2}+...+a_{n}^{2}}\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;Vector Addition&lt;/h3&gt;
&lt;p&gt;To add two vectors, add each corresponding element. For example, for vectors &lt;span class="math"&gt;\(\boldsymbol{a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{b}\)&lt;/span&gt;, the sum would be &lt;span class="math"&gt;\((a_{1}+b_{1}, a_{2}+b_{2}, ... a_{n}+b_{n})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To add the lengths of two vectors, the process is the same:&lt;/p&gt;
&lt;div class="math"&gt;$$\left \| \boldsymbol{a} + \boldsymbol{b} \right \| = \sqrt{(a_{1}+b_{1})^{2}+(a_{2}+b_{2})^{2}+...+(a_{n}+b_{n})^{2}}$$&lt;/div&gt;
&lt;h3&gt;Multiply A Scalar By A Vector&lt;/h3&gt;
&lt;p&gt;Simply multiply each element of the vector by the scalar. For example:&lt;/p&gt;
&lt;div class="math"&gt;$$c\boldsymbol{x} = (cx_{1}, cx_{2}, ... cx_{x})$$&lt;/div&gt;
&lt;h3&gt;Dot Product&lt;/h3&gt;
&lt;p&gt;One way of thinking about the dot product is that if two vectors are imagined as two arrows pointing out into a 2nd coordinate plane, the dot product is the vector starting at one endpoint and going to the other endpoint.&lt;/p&gt;
&lt;p&gt;Because if two vectors are perpendicular then the dot product is 0 (because the cosine of the angle they create is 0), the dot product is used to tell you if two vectors are perpendicular/orthogonal/independent.&lt;/p&gt;
&lt;div class="math"&gt;$$\boldsymbol{a} \cdot  \boldsymbol{b} = a_{1}b_{1} + a_{2}b_{2} + ... a_{n}b_{n}$$&lt;/div&gt;
&lt;h3&gt;Matrix&lt;/h3&gt;
&lt;p&gt;A matrix is a rectangular table of numbers of variables that contains rows and columns. It is typically described as an &lt;span class="math"&gt;\(n \times m\)&lt;/span&gt; matrix, meaning &lt;span class="math"&gt;\(n\)&lt;/span&gt; nows and &lt;span class="math"&gt;\(m\)&lt;/span&gt; columns.&lt;/p&gt;
&lt;div class="math"&gt;$$A_{3x3} = \begin{pmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; a_{31}\\
a_{21} &amp;amp;  a_{22}&amp;amp; a_{32}\\
a_{31} &amp;amp; a_{32} &amp;amp; a_{33}
\end{pmatrix}$$&lt;/div&gt;
&lt;h3&gt;Scalar Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;To multiply a scalar by a matrix, simply multiply each element of the matrix by the scalar.&lt;/p&gt;
&lt;div class="math"&gt;$$2 \times \begin{bmatrix}
1 &amp;amp; 2\\
3 &amp;amp; 4
\end{bmatrix} = \begin{bmatrix}
2 &amp;amp; 4\\
6 &amp;amp; 8
\end{bmatrix}$$&lt;/div&gt;
&lt;h3&gt;Matrix Multiplication&lt;/h3&gt;
&lt;p&gt;To multiply two matrices, it is simply the dot product of the each of first matrix's rows and the second matrix's columns.&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{bmatrix}
1 &amp;amp; 2\\
3 &amp;amp; 4\\
5 &amp;amp; 6
\end{bmatrix}\times  \begin{bmatrix}
1 &amp;amp;  2&amp;amp; 3\\ 4 &amp;amp;  5&amp;amp; 6
\end{bmatrix} = \begin{bmatrix}
(1\times1) + (2\times4)&amp;amp;  (1\times2) + (2\times5)&amp;amp; (1\times3) + (2\times6)\\
(3\times1) + (4\times4) &amp;amp;  (3\times2) + (4\times5)&amp;amp; (3\times3) + (4\times6)\\
(5\times1) + (6\times4) &amp;amp; (5\times2) + (6\times5) &amp;amp; (5\times3) + (6\times6)
\end{bmatrix} $$&lt;/div&gt;
&lt;h3&gt;Trace Of A Matrix&lt;/h3&gt;
&lt;p&gt;The trace of a matrix is the sum of it's diagonal elements.&lt;/p&gt;
&lt;h3&gt;Identity Matrix&lt;/h3&gt;
&lt;p&gt;A square matrix of &lt;span class="math"&gt;\(1\)&lt;/span&gt; on the diagonal elements and &lt;span class="math"&gt;\(0\)&lt;/span&gt; everywhere else.&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1
\end{bmatrix}$$&lt;/div&gt;
&lt;h3&gt;Inverse Of A Matrix&lt;/h3&gt;
&lt;p&gt;The inverse of a matrix, &lt;span class="math"&gt;\(\boldsymbol{A}\)&lt;/span&gt;, is a second matrix, &lt;span class="math"&gt;\(\boldsymbol{A}^{-1}\)&lt;/span&gt;, such that &lt;span class="math"&gt;\(\boldsymbol{A} \cdot \boldsymbol{A}^{-1} = \boldsymbol{I}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\boldsymbol{I}\)&lt;/span&gt; is an identity matrix.&lt;/p&gt;
&lt;h3&gt;Matrix Rank&lt;/h3&gt;
&lt;p&gt;The rank of a matrix is the maximum number of linearly independent rows or columns.&lt;/p&gt;
&lt;h3&gt;Eigenvalue and Eigenvector&lt;/h3&gt;
&lt;p&gt;An eigenvalue of a matrix is the solution to &lt;span class="math"&gt;\(A\boldsymbol{x}=\lambda\boldsymbol{x}\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; is called the eigenvector, and &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is called the eigenvalue.&lt;/p&gt;
&lt;p&gt;If the eigenvector &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; is multiplied by &lt;span class="math"&gt;\(A\)&lt;/span&gt;, then &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt; ends up as a scalar multiple of itself that always stays pointed in the same direction but just gets longer or shorter. The &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; tells us how much longer or shorter it is.&lt;/p&gt;
&lt;p&gt;In some cases like Markov Chains, researchers think of the eigenvector, &lt;span class="math"&gt;\(\boldsymbol{x}\)&lt;/span&gt;, as the state of the system, and &lt;span class="math"&gt;\(A\)&lt;/span&gt; as the transition matrix between states.&lt;/p&gt;
&lt;h3&gt;Markov Chains&lt;/h3&gt;
&lt;p&gt;Markov processes are memoryless, meaning there is no dependence between the present state and the past state. A Markov chain is a stochastic process that is memoryless.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Fri, 12 Aug 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-08-12:mathematics/basic_mathematical_terms_in_python.html</guid><category>Basics</category></item><item><title>Sort A List Of Names By Last Name</title><link>http://chrisalbon.com/python/sort_a_list_by_last_name.html</link><description>&lt;h2&gt;Create a list of names&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;commander_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alan Brooke&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;George Marshall&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Frank Jack Fletcher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Conrad Helfrich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Albert Kesselring&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sort Alphabetically By Last Name&lt;/h2&gt;
&lt;p&gt;To complete the sort, we will combine three operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lambda x: x.split(" ")&lt;/code&gt;, which is a function that takes a string &lt;code&gt;x&lt;/code&gt; and breaks it up along each blank space. This outputs a list.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[-1]&lt;/code&gt;, which takes the last element of a list.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sorted()&lt;/code&gt;, which sorts a list.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Sort a variable called &amp;#39;commander_names&amp;#39; by the last elements of each name.&lt;/span&gt;
&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;commander_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[&amp;#39;Alan Brooke&amp;#39;,
 &amp;#39;Frank Jack Fletcher&amp;#39;,
 &amp;#39;Conrad Helfrich&amp;#39;,
 &amp;#39;Albert Kesselring&amp;#39;,
 &amp;#39;George Marshall&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Sat, 25 Jun 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-06-25:python/sort_a_list_by_last_name.html</guid><category>Data Wrangling</category></item><item><title>Sort A List Of Strings By Length</title><link>http://chrisalbon.com/python/sort_a_list_of_strings_by_length.html</link><description>&lt;h2&gt;Create a list of names&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;commander_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Alan Brooke&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;George Marshall&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Frank Jack Fletcher&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Conrad Helfrich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Albert Kesselring&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sort Alphabetically By Length&lt;/h2&gt;
&lt;p&gt;To complete the sort, we will combine two operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lambda x: len(x)&lt;/code&gt;, which returns the length of each string.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sorted()&lt;/code&gt;, which sorts a list.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Sort a variable called &amp;#39;commander_names&amp;#39; by the length of each string&lt;/span&gt;
&lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;commander_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[&amp;#39;Alan Brooke&amp;#39;,
 &amp;#39;George Marshall&amp;#39;,
 &amp;#39;Conrad Helfrich&amp;#39;,
 &amp;#39;Albert Kesselring&amp;#39;,
 &amp;#39;Frank Jack Fletcher&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Albon</dc:creator><pubDate>Sat, 25 Jun 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:chrisalbon.com,2016-06-25:python/sort_a_list_of_strings_by_length.html</guid><category>Data Wrangling</category></item></channel></rss>